"""
train2.py
New training script that:
- builds training data including the original features (kept intact) plus new
  funneling_* and smurfing_* features computed by enrich2.py.
- trains an IsolationForest model and registers it under a NEW model name
  (AML_IF_v2) so your original model remains available.
- logs metrics to MLflow and saves a model artifact with a timestamp.
- extensive comments explain each step.

We do not NOT alter the original training datasets or models.
It creates a separate training run and model name (AML_IF_v2).
"""

import os
import sys
import logging
import pickle
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import mlflow
import mlflow.sklearn
from neo4j import GraphDatabase

# ----------------- CONFIG -----------------
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://192.168.0.5:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASS = os.getenv("NEO4J_PASS", "PotatoDTND12!")
MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000")
MODEL_NAME_V2 = os.getenv("MLFLOW_MODEL_NAME_V2", "AML_IF_v2")

LOG_DIR = os.getenv("LOG_DIR", "/var/log/aml_pipeline")
os.makedirs(LOG_DIR, exist_ok=True)
logging.basicConfig(filename=os.path.join(LOG_DIR, "train2.log"), level=logging.INFO)

driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))

# We'll follow existing training window convention: days_ago argument or default.
DEFAULT_DAYS_AGO = 1

# ----------------- DATA fetching -----------------
def get_training_window(days_ago):
    """
    Mirror your previous utility: compute start and end timestamps to fetch transactions
    to use for training. This function intentionally matches the contract in train.py.
    """
    end_dt = datetime.utcnow() - timedelta(days=days_ago-1)
    start_dt = end_dt - timedelta(days=30)  # train on last 30 days by default
    return int(start_dt.timestamp()), int(end_dt.timestamp())

def fetch_transactions_for_train(tx, start_ts, end_ts):
    """
    Fetch enriched transactions including the v2 features we wrote in enrich2.py.
    We keep a fall-back path if some v2 fields are missing (coalesce in cypher).
    """
    query = """
    MATCH (t:Transaction)
    WHERE t.timestamp >= $start_ts AND t.timestamp < $end_ts
      AND t.enriched_at IS NOT NULL  // ensure original enrichment done
    RETURN t.transaction_id AS transaction_id,
           coalesce(t.amount, 0.0) AS amount,
           coalesce(t.from_pagerank, 0.0) AS from_pagerank,
           coalesce(t.to_pagerank, 0.0) AS to_pagerank,
           coalesce(t.from_centrality, 0.0) AS from_centrality,
           coalesce(t.to_centrality, 0.0) AS to_centrality,
           coalesce(t.funneling_score_v2, 0.0) AS funneling_score_v2,
           coalesce(t.funneling_multiple_senders_v2, 0) AS funneling_multiple_senders_v2,
           coalesce(t.smurfing_score_v2, 0.0) AS smurfing_score_v2,
           coalesce(t.smurfing_small_tx_count_v2, 0) AS smurfing_small_tx_count_v2
    """
    results = tx.run(query, start_ts=start_ts, end_ts=end_ts)
    rows = []
    for r in results:
        rows.append(dict(r))
    return pd.DataFrame(rows)

# ----------------- PREPROCESS -----------------
def preprocess_dataframe(df):
    """
    - Drop any rows with missing core numeric fields (defensive).
    - Build feature matrix in a stable column order. Keep original feature columns
      untouched so the old training runs are not affected.
    - Standardize features using StandardScaler and return scaler for later use.
    """
    required = ["amount", "from_pagerank", "to_pagerank", "from_centrality", "to_centrality"]
    df = df.dropna(subset=required)
    feature_cols = [
        "amount",
        "from_pagerank",
        "to_pagerank",
        "from_centrality",
        "to_centrality",
        # new v2 features
        "funneling_score_v2",
        "funneling_multiple_senders_v2",
        "smurfing_score_v2",
        "smurfing_small_tx_count_v2"
    ]
    features = df[feature_cols].astype(float)
    scaler = StandardScaler()
    X = scaler.fit_transform(features)
    return df, X, scaler, feature_cols

# ----------------- TRAIN & REGISTER -----------------
def train_isolation_forest(X, n_estimators=200, contamination="auto", random_state=42):
    """
    Train a fresh IsolationForest. We keep a stronger ensemble (200 trees) for v2.
    """
    clf = IsolationForest(
        n_estimators=n_estimators,
        contamination=contamination,
        random_state=random_state,
        n_jobs=-1
    )
    clf.fit(X)
    return clf

def register_model_mlflow(model, scaler, feature_cols, run_name="train_v2"):
    """
    Use mlflow to log model, scaler, and metadata, and register it under MODEL_NAME_V2.
    """
    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    mlflow.set_experiment("AML IsolationForest Training v2")
    with mlflow.start_run(run_name=run_name) as run:
        mlflow.log_param("n_features", len(feature_cols))
        mlflow.log_param("feature_columns", json.dumps(feature_cols))
        # Save scaler + model as artifact
        artifact_path = "model_artifact"
        os.makedirs("/tmp/model_artifact", exist_ok=True)
        with open("/tmp/model_artifact/scaler.pkl", "wb") as f:
            pickle.dump(scaler, f)
        mlflow.sklearn.log_model(model, artifact_path)
        mlflow.log_artifact("/tmp/model_artifact/scaler.pkl")
        # Register model under new name
        mlflow.register_model(f"runs:/{run.info.run_id}/{artifact_path}", MODEL_NAME_V2)
        logging.info("Registered model %s (run_id=%s)", MODEL_NAME_V2, run.info.run_id)
        return run.info.run_id

# ----------------- MAIN -----------------
if __name__ == "__main__":
    try:
        days_ago = int(sys.argv[1])
    except (IndexError, ValueError):
        days_ago = DEFAULT_DAYS_AGO

    start_ts, end_ts = get_training_window(days_ago)
    logging.info("Training window: %s -> %s", start_ts, end_ts)

    with driver.session() as session:
        df = session.read_transaction(fetch_transactions_for_train, start_ts, end_ts)

    if df.empty:
        logging.error("No training data found; aborting")
        sys.exit(1)

    df, X, scaler, feature_cols = preprocess_dataframe(df)
    # Optionally sample / augment here (mirror old pipeline)
    # We'll follow previous percentages: sample 10% real and add 10% synthetic if desired.
    sample_size = max(1, int(0.10 * len(df)))
    real_sample = df.sample(n=sample_size, random_state=42)
    logging.info("Sampled %d records for training", sample_size)

    # For v2 we re-use the original synthetic generator if present; else we train on sample only
    # (keeping this code small — you can reuse your original generate_synthetic_data if desired)
    X_sample = scaler.transform(real_sample[feature_cols].astype(float))

    model = train_isolation_forest(X_sample)
    run_id = register_model_mlflow(model, scaler, feature_cols, run_name=f"train_v2_{int(datetime.utcnow().timestamp())}")

    logging.info("✅ Training v2 complete. Run id: %s", run_id)
    print("✅ Training v2 complete. Run id:", run_id)
