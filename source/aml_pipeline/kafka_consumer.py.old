import json
import logging
import multiprocessing
from datetime import datetime, timedelta
from confluent_kafka import Consumer, KafkaException
from neo4j import GraphDatabase
import networkx as nx

# Configuration
KAFKA_TOPIC = "usdc-transactions"
KAFKA_BROKER = "192.168.0.4:9092"
KAFKA_GROUP = "aml_pipeline_consumer"
NEO4J_URI = "bolt://192.168.0.5:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "PotatoDTND12!"
SLIDING_WINDOW_DAYS = 1

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)

driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

def enrich_wallet_features(tx):
    try:
        # CHECK APPROPRIATE SLIDING WINDOW
        cutoff = int((datetime.now() - timedelta(days=SLIDING_WINDOW_DAYS)).timestamp())

        query = """
            MATCH (w1:Wallet)-[s:SENT]->(tx:Transaction)-[r:RECEIVED_BY]->(w2:Wallet)
            WHERE tx.timestamp >= $cutoff
            RETURN w1.address AS from_address, w2.address AS to_address
        """
        results = tx.run(query, cutoff=cutoff)

        G = nx.DiGraph()
        for record in results:
            G.add_edge(record["from_address"], record["to_address"])

        return nx.pagerank(G), nx.degree_centrality(G)
    except Exception as e:
        logging.warning("Graph enrichment failed: %s", e)
        return {}, {}

def process_transaction(data):
    with driver.session() as session:
        pagerank, centrality = session.read_transaction(enrich_wallet_features)
        session.write_transaction(_write_tx, data, pagerank, centrality)

def _write_tx(tx, data, pagerank, centrality):
    tx_id = data["transaction_id"]
    from_addr = data["from_address"]
    to_addr = data["to_address"]

    data.update({
        "from_pagerank": pagerank.get(from_addr, 0.0),
        "to_pagerank": pagerank.get(to_addr, 0.0),
        "from_centrality": centrality.get(from_addr, 0.0),
        "to_centrality": centrality.get(to_addr, 0.0),
        "processed_at": int(datetime.utcnow().timestamp())
    })

    tx.run("""
        MERGE (tx:Transaction {transaction_id: $transaction_id})
        SET tx += $tx_data

        MERGE (from_wallet:Wallet {address: $from_address})
        MERGE (to_wallet:Wallet {address: $to_address})

        MERGE (from_wallet)-[:SENT]->(tx)
        MERGE (tx)-[:RECEIVED_BY]->(to_wallet)
    """, transaction_id=tx_id, tx_data=data, from_address=from_addr, to_address=to_addr)

def worker_process(worker_id):
    consumer = Consumer({
        "bootstrap.servers": KAFKA_BROKER,
        "group.id": KAFKA_GROUP,
        "auto.offset.reset": "earliest",
        "enable.auto.commit": False,
        "session.timeout.ms": 6000
    })
    consumer.subscribe([KAFKA_TOPIC])

    logging.info(f"ðŸ‘· Worker {worker_id} started")
    try:
        while True:
            msg = consumer.poll(1.0)
            if msg is None:
                continue
            if msg.error():
                raise KafkaException(msg.error())

            data = json.loads(msg.value().decode("utf-8"))
            process_transaction(data)
            consumer.commit()
            logging.info(f"âœ… Worker {worker_id} processed: {data['transaction_id']}")
    finally:
        consumer.close()

if __name__ == "__main__":
    num_workers = 8
    pool = multiprocessing.Pool(processes=num_workers)
    pool.map(worker_process, range(num_workers))
