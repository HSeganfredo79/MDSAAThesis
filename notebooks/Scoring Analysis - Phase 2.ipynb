{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac332b7",
   "metadata": {},
   "source": [
    "# Phase 2 — Business-Aware Enrichment (Funneling & Smurfing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8fe5287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup OK.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "NEO4J_BOLT_URI = \"bolt://192.168.0.5:7687\"\n",
    "NEO4J_AUTH     = (\"neo4j\", \"PotatoDTND12!\")\n",
    "driver = GraphDatabase.driver(NEO4J_BOLT_URI, auth=NEO4J_AUTH)\n",
    "\n",
    "APRIL_START_EPOCH = 1743476400\n",
    "APRIL_END_EPOCH   = 1746068400\n",
    "APRIL1_START      = 1743476400\n",
    "APRIL1_END        = 1743562800\n",
    "\n",
    "print(\"Setup OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15435f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _run_single(s, q, **params):\n",
    "    res = s.run(q, **params)\n",
    "    rec = res.single()\n",
    "    return rec.data() if rec else None\n",
    "\n",
    "def _default_stats_dict():\n",
    "    return {\"n_total\": 0, \"mean_score\": None, \"std_score\": None, \"anomaly_rate\": 0.0,\n",
    "            \"anom_min\": None, \"anom_max\": None, \"norm_min\": None, \"norm_max\": None}\n",
    "\n",
    "def one_minus_ws(df_scores):\n",
    "    if df_scores.empty or df_scores[\"label\"].nunique() < 2:\n",
    "        return {\"one_minus_ws\": np.nan, \"ws\": np.nan, \"clarity\": \"unclear\"}\n",
    "    a = df_scores.loc[df_scores.label==1, \"score\"].astype(float).values\n",
    "    b = df_scores.loc[df_scores.label==0, \"score\"].astype(float).values\n",
    "    if len(a)==0 or len(b)==0:\n",
    "        return {\"one_minus_ws\": np.nan, \"ws\": np.nan, \"clarity\": \"unclear\"}\n",
    "    ws = wasserstein_distance(a, b)\n",
    "    lo = float(np.nanmin(np.concatenate([a,b])))\n",
    "    hi = float(np.nanmax(np.concatenate([a,b])))\n",
    "    rng = max(hi - lo, 1e-12)\n",
    "    ws_norm = ws / rng\n",
    "    sim = 1.0 - max(0.0, min(1.0, ws_norm))\n",
    "    clarity = \"clear\" if ws_norm >= 0.2 else \"unclear\"\n",
    "    return {\"one_minus_ws\": sim, \"ws\": ws_norm, \"clarity\": clarity}\n",
    "\n",
    "def sci(x, sig=6):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"n/a\"\n",
    "    if x == 0:\n",
    "        return f\"{0:.{sig}f} × 10^0\"\n",
    "    exp = int(np.floor(np.log10(abs(x))))\n",
    "    mant = x / (10**exp)\n",
    "    return f\"{mant:.{sig}f} × 10^{exp}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb29289",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stats_global_phase1(start_epoch, end_epoch):\n",
    "    q = \"\"\"\n",
    "MATCH (tx:Transaction)\n",
    "WHERE tx.timestamp >= $startEpoch AND tx.timestamp < $endEpoch\n",
    "  AND tx.label IS NOT NULL AND tx.scoring IS NOT NULL\n",
    "RETURN\n",
    "  count(tx)                                                 AS n_total,\n",
    "  avg(toFloat(tx.scoring))                                  AS mean_score,\n",
    "  stDev(toFloat(tx.scoring))                                AS std_score,\n",
    "  toFloat(sum(CASE WHEN toInteger(tx.label)=1 THEN 1 ELSE 0 END))/count(tx) AS anomaly_rate,\n",
    "  min(CASE WHEN toInteger(tx.label)=1 THEN toFloat(tx.scoring) END)         AS anom_min,\n",
    "  max(CASE WHEN toInteger(tx.label)=1 THEN toFloat(tx.scoring) END)         AS anom_max,\n",
    "  min(CASE WHEN toInteger(tx.label)=0 THEN toFloat(tx.scoring) END)         AS norm_min,\n",
    "  max(CASE WHEN toInteger(tx.label)=0 THEN toFloat(tx.scoring) END)         AS norm_max\n",
    "\"\"\"\n",
    "    with driver.session() as s:\n",
    "        rec = _run_single(s, q, startEpoch=start_epoch, endEpoch=end_epoch)\n",
    "    return rec or _default_stats_dict()\n",
    "\n",
    "def stats_global_phase2(start_epoch, end_epoch):\n",
    "    q = \"\"\"\n",
    "MATCH (tx:Transaction)\n",
    "WHERE tx.timestamp >= $startEpoch AND tx.timestamp < $endEpoch\n",
    "  AND tx.label_v2 IS NOT NULL AND tx.scoring_v2 IS NOT NULL\n",
    "RETURN\n",
    "  count(tx)                                                  AS n_total,\n",
    "  avg(toFloat(tx.scoring_v2))                                AS mean_score,\n",
    "  stDev(toFloat(tx.scoring_v2))                              AS std_score,\n",
    "  toFloat(sum(CASE WHEN toInteger(tx.label_v2)=1 THEN 1 ELSE 0 END))/count(tx) AS anomaly_rate,\n",
    "  min(CASE WHEN toInteger(tx.label_v2)=1 THEN toFloat(tx.scoring_v2) END)       AS anom_min,\n",
    "  max(CASE WHEN toInteger(tx.label_v2)=1 THEN toFloat(tx.scoring_v2) END)       AS anom_max,\n",
    "  min(CASE WHEN toInteger(tx.label_v2)=0 THEN toFloat(tx.scoring_v2) END)       AS norm_min,\n",
    "  max(CASE WHEN toInteger(tx.label_v2)=0 THEN toFloat(tx.scoring_v2) END)       AS norm_max\n",
    "\"\"\"\n",
    "    with driver.session() as s:\n",
    "        rec = _run_single(s, q, startEpoch=start_epoch, endEpoch=end_epoch)\n",
    "    return rec or _default_stats_dict()\n",
    "\n",
    "def sample_scores_global_phase1(start_epoch, end_epoch, limit_per_class=40000):\n",
    "    q = \"\"\"\n",
    "MATCH (tx:Transaction)\n",
    "WHERE tx.timestamp >= $startEpoch AND tx.timestamp < $endEpoch\n",
    "  AND tx.label IS NOT NULL AND tx.scoring IS NOT NULL\n",
    "WITH toInteger(tx.label) AS y, toFloat(tx.scoring) AS s\n",
    "ORDER BY rand()\n",
    "WITH y, collect(s)[0..$cap] AS ss\n",
    "RETURN y AS label, ss AS scores\n",
    "\"\"\"\n",
    "    rows = []\n",
    "    cap = int(limit_per_class)\n",
    "    with driver.session() as s:\n",
    "        for r in s.run(q, startEpoch=start_epoch, endEpoch=end_epoch, cap=cap):\n",
    "            rows.append((r[\"label\"], r[\"scores\"]))\n",
    "    data = []\n",
    "    for label, scores in rows:\n",
    "        for sc in scores:\n",
    "            data.append({\"label\": int(label), \"score\": float(sc)})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def sample_scores_global_phase2(start_epoch, end_epoch, limit_per_class=40000):\n",
    "    q = \"\"\"\n",
    "MATCH (tx:Transaction)\n",
    "WHERE tx.timestamp >= $startEpoch AND tx.timestamp < $endEpoch\n",
    "  AND tx.label_v2 IS NOT NULL AND tx.scoring_v2 IS NOT NULL\n",
    "WITH toInteger(tx.label_v2) AS y, toFloat(tx.scoring_v2) AS s\n",
    "ORDER BY rand()\n",
    "WITH y, collect(s)[0..$cap] AS ss\n",
    "RETURN y AS label, ss AS scores\n",
    "\"\"\"\n",
    "    rows = []\n",
    "    cap = int(limit_per_class)\n",
    "    with driver.session() as s:\n",
    "        for r in s.run(q, startEpoch=start_epoch, endEpoch=end_epoch, cap=cap):\n",
    "            rows.append((r[\"label\"], r[\"scores\"]))\n",
    "    data = []\n",
    "    for label, scores in rows:\n",
    "        for sc in scores:\n",
    "            data.append({\"label\": int(label), \"score\": float(sc)})\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b37f05ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\nMATCH (tx:Transaction)\\nWHERE tx.timestamp >= $startEpoch AND tx.timestamp < $endEpoch\\n  AND tx.label IS NOT NULL AND tx.scoring IS NOT NULL\\nRETURN\\n  count(tx)                                                 AS n_total,\\n  avg(toFloat(tx.scoring))                                  AS mean_score,\\n  stDev(toFloat(tx.scoring))                                AS std_score,\\n  toFloat(sum(CASE WHEN toInteger(tx.label)=1 THEN 1 ELSE 0 END))/count(tx) AS anomaly_rate,\\n  min(CASE WHEN toInteger(tx.label)=1 THEN toFloat(tx.scoring) END)         AS anom_min,\\n  max(CASE WHEN toInteger(tx.label)=1 THEN toFloat(tx.scoring) END)         AS anom_max,\\n  min(CASE WHEN toInteger(tx.label)=0 THEN toFloat(tx.scoring) END)         AS norm_min,\\n  max(CASE WHEN toInteger(tx.label)=0 THEN toFloat(tx.scoring) END)         AS norm_max\\n'\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\nMATCH (tx:Transaction)\\nWHERE tx.timestamp >= $startEpoch AND tx.timestamp < $endEpoch\\n  AND tx.label IS NOT NULL AND tx.scoring IS NOT NULL\\nRETURN\\n  count(tx)                                                 AS n_total,\\n  avg(toFloat(tx.scoring))                                  AS mean_score,\\n  stDev(toFloat(tx.scoring))                                AS std_score,\\n  toFloat(sum(CASE WHEN toInteger(tx.label)=1 THEN 1 ELSE 0 END))/count(tx) AS anomaly_rate,\\n  min(CASE WHEN toInteger(tx.label)=1 THEN toFloat(tx.scoring) END)         AS anom_min,\\n  max(CASE WHEN toInteger(tx.label)=1 THEN toFloat(tx.scoring) END)         AS anom_max,\\n  min(CASE WHEN toInteger(tx.label)=0 THEN toFloat(tx.scoring) END)         AS norm_min,\\n  max(CASE WHEN toInteger(tx.label)=0 THEN toFloat(tx.scoring) END)         AS norm_max\\n'\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\nMATCH (tx:Transaction)\\nWHERE tx.timestamp >= $startEpoch AND tx.timestamp < $endEpoch\\n  AND tx.label_v2 IS NOT NULL AND tx.scoring_v2 IS NOT NULL\\nRETURN\\n  count(tx)                                                  AS n_total,\\n  avg(toFloat(tx.scoring_v2))                                AS mean_score,\\n  stDev(toFloat(tx.scoring_v2))                              AS std_score,\\n  toFloat(sum(CASE WHEN toInteger(tx.label_v2)=1 THEN 1 ELSE 0 END))/count(tx) AS anomaly_rate,\\n  min(CASE WHEN toInteger(tx.label_v2)=1 THEN toFloat(tx.scoring_v2) END)       AS anom_min,\\n  max(CASE WHEN toInteger(tx.label_v2)=1 THEN toFloat(tx.scoring_v2) END)       AS anom_max,\\n  min(CASE WHEN toInteger(tx.label_v2)=0 THEN toFloat(tx.scoring_v2) END)       AS norm_min,\\n  max(CASE WHEN toInteger(tx.label_v2)=0 THEN toFloat(tx.scoring_v2) END)       AS norm_max\\n'\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\nMATCH (tx:Transaction)\\nWHERE tx.timestamp >= $startEpoch AND tx.timestamp < $endEpoch\\n  AND tx.label_v2 IS NOT NULL AND tx.scoring_v2 IS NOT NULL\\nRETURN\\n  count(tx)                                                  AS n_total,\\n  avg(toFloat(tx.scoring_v2))                                AS mean_score,\\n  stDev(toFloat(tx.scoring_v2))                              AS std_score,\\n  toFloat(sum(CASE WHEN toInteger(tx.label_v2)=1 THEN 1 ELSE 0 END))/count(tx) AS anomaly_rate,\\n  min(CASE WHEN toInteger(tx.label_v2)=1 THEN toFloat(tx.scoring_v2) END)       AS anom_min,\\n  max(CASE WHEN toInteger(tx.label_v2)=1 THEN toFloat(tx.scoring_v2) END)       AS anom_max,\\n  min(CASE WHEN toInteger(tx.label_v2)=0 THEN toFloat(tx.scoring_v2) END)       AS norm_min,\\n  max(CASE WHEN toInteger(tx.label_v2)=0 THEN toFloat(tx.scoring_v2) END)       AS norm_max\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2 global comparisons ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run\n",
    "p1_apr1_stats = stats_global_phase1(APRIL1_START, APRIL1_END)\n",
    "p1_apr_stats  = stats_global_phase1(APRIL_START_EPOCH, APRIL_END_EPOCH)\n",
    "p1_apr1_samp  = sample_scores_global_phase1(APRIL1_START, APRIL1_END)\n",
    "p1_apr_samp   = sample_scores_global_phase1(APRIL_START_EPOCH, APRIL_END_EPOCH)\n",
    "p1_apr1_sim   = one_minus_ws(p1_apr1_samp)\n",
    "p1_apr_sim    = one_minus_ws(p1_apr_samp)\n",
    "\n",
    "p2_apr1_stats = stats_global_phase2(APRIL1_START, APRIL1_END)\n",
    "p2_apr_stats  = stats_global_phase2(APRIL_START_EPOCH, APRIL_END_EPOCH)\n",
    "p2_apr1_samp  = sample_scores_global_phase2(APRIL1_START, APRIL1_END)\n",
    "p2_apr_samp   = sample_scores_global_phase2(APRIL_START_EPOCH, APRIL_END_EPOCH)\n",
    "p2_apr1_sim   = one_minus_ws(p2_apr1_samp)\n",
    "p2_apr_sim    = one_minus_ws(p2_apr_samp)\n",
    "\n",
    "print(\"Phase 2 global comparisons ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b017df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nevertheless, we started a new enrichment phase which added funneling and smurfing signaling features over all the April 2025 traffic, revealed a clear separation between anomaly and normal scores. As Table 4-10 shows, cases tied to these typologies began resulting in distinct score profiles. These behaviors were flagged from the enriched features capturing graph topology asymmetries (in degree vs. out degree) and temporal density (transaction bursts) around nodes.\n",
      "\n",
      "Also, as shown in Table 4-11, the 1−WS similarity changed from 0.4646 to 0.4446 (WS distance from 0.5354 to 0.5554), supporting the intuition that business aware features (funneling and smurfing-related) better inform anomaly detection than agnostic (graph rank and centrality) attributes alone.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Word switches (based on WS change and absolute WS in Phase 2)\n",
    "def switches(p1_sim, p2_sim):\n",
    "    clarity = \"clear\" if p2_sim[\"ws\"] >= 0.2 else \"unclear\"\n",
    "    profiles = \"distinct\" if p2_sim[\"ws\"] >= 0.2 else \"indistinct\"\n",
    "    delta = (p2_sim[\"ws\"] - p1_sim[\"ws\"]) if (p1_sim[\"ws\"] is not np.nan and p2_sim[\"ws\"] is not np.nan) else 0.0\n",
    "    benefit = \"flagged\" if delta >= 0.02 else \"did not benefit\"\n",
    "    support = \"supporting\" if delta >= 0.02 else \"unsupporting\"\n",
    "    return clarity, profiles, benefit, support, delta\n",
    "\n",
    "apr1_clarity, apr1_profiles, apr1_benefit, apr1_support, apr1_delta = switches(p1_apr1_sim, p2_apr1_sim)\n",
    "apr_clarity,  apr_profiles,  apr_benefit,  apr_support,  apr_delta  = switches(p1_apr_sim,  p2_apr_sim)\n",
    "\n",
    "text = f\"\"\"Nevertheless, we started a new enrichment phase which added funneling and smurfing signaling features over all the April 2025 traffic, revealed a {apr_clarity} separation between anomaly and normal scores. As Table 4-10 shows, cases tied to these typologies began resulting in {apr_profiles} score profiles. These behaviors were {apr_benefit} from the enriched features capturing graph topology asymmetries (in degree vs. out degree) and temporal density (transaction bursts) around nodes.\n",
    "\n",
    "Also, as shown in Table 4-11, the 1−WS similarity changed from {1.0 - p1_apr_sim['ws']:.4f} to {1.0 - p2_apr_sim['ws']:.4f} (WS distance from {p1_apr_sim['ws']:.4f} to {p2_apr_sim['ws']:.4f}), {apr_support} the intuition that business aware features (funneling and smurfing-related) better inform anomaly detection than agnostic (graph rank and centrality) attributes alone.\"\"\"\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e322086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 4-10 — Anomaly score separation after business-aware enrichment (Phase 2)\n",
      "        Period  Anomaly Rate Anomaly Min Score Anomaly Max Score Normal Min Score Normal Max Score\n",
      "April 1st 2025      0.062016 -1.374533 × 10^-1 -4.825527 × 10^-6 1.901936 × 10^-5 2.693719 × 10^-1\n",
      "    April 2025      0.051436 -1.921282 × 10^-1 -4.825527 × 10^-6 5.361697 × 10^-7 2.693719 × 10^-1\n",
      "\n",
      "Table 4-11 — 1−WS similarity and WS distance after business-aware enrichment\n",
      "        Period  1−WS (Phase 1)  1−WS (Phase 2)  WS Dist (Phase 1)  WS Dist (Phase 2)  Δ WS Dist (P2−P1)\n",
      "April 1st 2025        0.409795        0.370421           0.590205           0.629579           0.039375\n",
      "    April 2025        0.464608        0.444580           0.535392           0.555420           0.020028\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Table 4-10 — Phase 2 stats\n",
    "tbl_410 = pd.DataFrame([\n",
    "    {\"Period\":\"April 1st 2025\",\n",
    "     \"Anomaly Rate\": p2_apr1_stats.get(\"anomaly_rate\"),\n",
    "     \"Anomaly Min Score\": sci(p2_apr1_stats.get(\"anom_min\")),\n",
    "     \"Anomaly Max Score\": sci(p2_apr1_stats.get(\"anom_max\")),\n",
    "     \"Normal Min Score\":  sci(p2_apr1_stats.get(\"norm_min\")),\n",
    "     \"Normal Max Score\":  sci(p2_apr1_stats.get(\"norm_max\"))},\n",
    "    {\"Period\":\"April 2025\",\n",
    "     \"Anomaly Rate\": p2_apr_stats.get(\"anomaly_rate\"),\n",
    "     \"Anomaly Min Score\": sci(p2_apr_stats.get(\"anom_min\")),\n",
    "     \"Anomaly Max Score\": sci(p2_apr_stats.get(\"anom_max\")),\n",
    "     \"Normal Min Score\":  sci(p2_apr_stats.get(\"norm_min\")),\n",
    "     \"Normal Max Score\":  sci(p2_apr_stats.get(\"norm_max\"))},\n",
    "])\n",
    "print(\"Table 4-10 — Anomaly score separation after business-aware enrichment (Phase 2)\")\n",
    "print(tbl_410.to_string(index=False))\n",
    "#tbl_410.to_csv(\"/mnt/data/Table_4_10_Phase2_global.csv\", index=False)\n",
    "\n",
    "# Table 4-11 — 1−WS stats\n",
    "tbl_411 = pd.DataFrame([\n",
    "    {\"Period\":\"April 1st 2025\",\n",
    "     \"1−WS (Phase 1)\": 1.0 - p1_apr1_sim[\"ws\"],\n",
    "     \"1−WS (Phase 2)\": 1.0 - p2_apr1_sim[\"ws\"],\n",
    "     \"WS Dist (Phase 1)\": p1_apr1_sim[\"ws\"],\n",
    "     \"WS Dist (Phase 2)\": p2_apr1_sim[\"ws\"],\n",
    "     \"Δ WS Dist (P2−P1)\": p2_apr1_sim[\"ws\"] - p1_apr1_sim[\"ws\"]},\n",
    "    {\"Period\":\"April 2025\",\n",
    "     \"1−WS (Phase 1)\": 1.0 - p1_apr_sim[\"ws\"],\n",
    "     \"1−WS (Phase 2)\": 1.0 - p2_apr_sim[\"ws\"],\n",
    "     \"WS Dist (Phase 1)\": p1_apr_sim[\"ws\"],\n",
    "     \"WS Dist (Phase 2)\": p2_apr_sim[\"ws\"],\n",
    "     \"Δ WS Dist (P2−P1)\": p2_apr_sim[\"ws\"] - p1_apr_sim[\"ws\"]},\n",
    "])\n",
    "print(\"\\nTable 4-11 — 1−WS similarity and WS distance after business-aware enrichment\")\n",
    "print(tbl_411.to_string(index=False))\n",
    "#tbl_411.to_csv(\"/mnt/data/Table_4_11_1WS_comparison.csv\", index=False)\n",
    "#print(\"\\nSaved: /mnt/data/Table_4_10_Phase2_global.csv and /mnt/data/Table_4_11_1WS_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b7949-2635-4aab-90aa-6e384a3a291d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
